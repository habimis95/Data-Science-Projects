{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Requiment:</strong>\n",
    "Build a reviewer filter. Use the various NLP tools and a new classifier, Naive Bayes, to predict if one reviewText is like (overall>=4)/ don't like (overall<=2)/ neutral (2<overall<4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp_musical').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json('Musical_Instruments_5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|1384719342|  [0, 0]|    5.0|Not much to write...|02 28, 2014|A2IBPI20UZIR0U|cassandra tu \"Yea...|                good|    1393545600|\n",
      "|1384719342|[13, 14]|    5.0|The product does ...|03 16, 2013|A14VAT5EAX3D9S|                Jake|                Jake|    1363392000|\n",
      "|1384719342|  [1, 1]|    5.0|The primary job o...|08 28, 2013|A195EZSQDW3E21|Rick Bennette \"Ri...|It Does The Job Well|    1377648000|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('class', when(data.overall >=4, 'like')\n",
    "                       .when(data.overall <=2, 'not_like')\n",
    "                       .otherwise('neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('reviewText', 'overall', 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data\n",
    "\n",
    "** Create a new length feature: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length', length(data['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+------+\n",
      "|          reviewText|overall|class|length|\n",
      "+--------------------+-------+-----+------+\n",
      "|Not much to write...|    5.0| like|   268|\n",
      "|The product does ...|    5.0| like|   544|\n",
      "|The primary job o...|    5.0| like|   436|\n",
      "|Nice windscreen p...|    5.0| like|   206|\n",
      "|This pop filter i...|    5.0| like|   159|\n",
      "+--------------------+-------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------------+\n",
      "|   class|      avg(overall)|      avg(length)|\n",
      "+--------+------------------+-----------------+\n",
      "|not_like|1.5353319057815846|579.2055674518201|\n",
      "| neutral|               3.0|579.2111398963731|\n",
      "|    like|4.7690090888938155|473.1188206606074|\n",
      "+--------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pretty Clear Difference\n",
    "data.groupby('class').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StringIndexer\n",
    "tokenizer = Tokenizer(inputCol = 'reviewText', outputCol = 'token_text')\n",
    "stopremove = StopWordsRemover(inputCol = 'token_text', outputCol = 'stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol = 'stop_tokens', outputCol = 'c_vec')\n",
    "idf = IDF(inputCol = 'c_vec', outputCol = 'tf_idf')\n",
    "class_to_num = StringIndexer(inputCol = 'class', outputCol = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols = ['tf_idf', 'length'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "We'll use the Naive Bayes, but feel free to play around with this choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defaults\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages = [class_to_num, tokenizer,\n",
    "                                    stopremove, count_vec,\n",
    "                                    idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(51949,[3,12,14,3...|\n",
      "|  0.0|(51949,[2,3,12,16...|\n",
      "|  0.0|(51949,[11,19,44,...|\n",
      "|  0.0|(51949,[18,37,57,...|\n",
      "|  0.0|(51949,[2,122,132...|\n",
      "|  0.0|(51949,[0,5,15,21...|\n",
      "|  0.0|(51949,[5,16,29,1...|\n",
      "|  1.0|(51949,[1,3,4,8,1...|\n",
      "|  0.0|(51949,[0,3,12,33...|\n",
      "|  0.0|(51949,[1,6,15,52...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show(10) # 0: like, 1: neutral, 2: not_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = clean_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 6346|\n",
      "|  1.0|  538|\n",
      "|  2.0|  338|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 2676|\n",
      "|  1.0|  234|\n",
      "|  2.0|  129|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- class: string (nullable = false)\n",
      " |-- length: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-11979.857561335...|[1.0,5.6454643749...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-4675.7072524771...|[1.0,5.2495428198...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-37635.488680969...|[1.59017540787312...|       1.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-2473.5167646879...|[1.0,1.1335949862...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-19030.000568201...|[4.78649391243439...|       1.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-10805.876284528...|[1.0,3.9312112275...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-22287.133862823...|[2.89739333812114...|       1.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-12508.387129341...|[1.0,1.8476477860...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-3930.8732605327...|[0.99999999956642...|       0.0|\n",
      "|  0.0|(51949,[0,1,2,3,4...|[-9467.8148477069...|[1.0,1.0344361900...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|   54|\n",
      "|  1.0|       1.0|   73|\n",
      "|  0.0|       1.0|  540|\n",
      "|  1.0|       0.0|  136|\n",
      "|  2.0|       2.0|   34|\n",
      "|  2.0|       1.0|   41|\n",
      "|  1.0|       2.0|   25|\n",
      "|  0.0|       0.0| 1972|\n",
      "|  0.0|       2.0|  164|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7386979737665652\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print('Accuracy of model at predicting: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not very good result (~74%)\n",
    "- Solution: Try switching out the classification models! Or even try to come up with other engineered features!..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LogisticRegression/RandomForest\n",
    "\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(maxIter = 20, regParam = 0.3, elasticNetParam = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_1 = lg.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_1 = predictor_1.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|  124|\n",
      "|  0.0|       1.0|    5|\n",
      "|  1.0|       0.0|  234|\n",
      "|  2.0|       2.0|    2|\n",
      "|  2.0|       1.0|    3|\n",
      "|  0.0|       0.0| 2668|\n",
      "|  0.0|       2.0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_1.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.8252989953949459\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_1 = acc_eval.evaluate(test_results_1)\n",
    "print('Accuracy of model at predicting: {}'.format(acc_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Higher accuracy but not better result!!!\n",
    "\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol = 'label', \\\n",
    "                             featuresCol = 'features', \\\n",
    "                             numTrees = 500, \\\n",
    "                             maxDepth = 5, \\\n",
    "                             maxBins = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2 = rfc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_2 = predictor_2.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|  129|\n",
      "|  1.0|       0.0|  234|\n",
      "|  0.0|       0.0| 2676|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_2.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 3039|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.8246226872183917\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_2 = acc_eval.evaluate(test_results_2)\n",
    "print('Accuracy of model at predicting: {}'.format(acc_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Higher accuracy but too bad result!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio like/neutral: 11\n",
      "ratio like/neutral: 18\n"
     ]
    }
   ],
   "source": [
    "like_df = training.filter(col('label') == 0)\n",
    "neutral_df = training.filter(col('label') == 1)\n",
    "not_like_df = training.filter(col('label') == 2)\n",
    "ratio_1 = int(like_df.count()/neutral_df.count())\n",
    "ratio_2 = int(like_df.count()/not_like_df.count())\n",
    "print('ratio like/neutral: {}'.format(ratio_1))\n",
    "print('ratio like/neutral: {}'.format(ratio_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(51949,[0],[1.025...|\n",
      "|  0.0|(51949,[0],[1.025...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# resample neutral\n",
    "a1 = range(ratio_1)\n",
    "# duplicate the minority rows\n",
    "oversampled_neutral_df = neutral_df.withColumn(\"dummy\", explode(array([lit(x) for x in a1]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows\n",
    "combined_df = like_df.unionAll(oversampled_neutral_df)\n",
    "combined_df.show(10)                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 6346|\n",
      "|  1.0| 5918|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(51949,[0],[1.025...|\n",
      "|  0.0|(51949,[0],[1.025...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "|  0.0|(51949,[0,1,2,3,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# resample not_like\n",
    "a2 = range(ratio_2)\n",
    "# duplicate the minority rows\n",
    "oversampled_notlike_df = not_like_df.withColumn(\"dummy\", explode(array([lit(x) for x in a2]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows\n",
    "combined_df = like_df.unionAll(oversampled_notlike_df)\n",
    "combined_df.show(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 6346|\n",
      "|  2.0| 6084|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_3 = nb.fit(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_3 = predictor_3.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 2676|\n",
      "|  1.0|  234|\n",
      "|  2.0|  129|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_3.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.8239115779815323\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_3 = acc_eval.evaluate(test_results_3)\n",
    "print('Accuracy of model at predicting: {}'.format(acc_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_4 = lg.fit(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_4 = predictor_4.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|  106|\n",
      "|  1.0|       0.0|  230|\n",
      "|  2.0|       2.0|   23|\n",
      "|  1.0|       2.0|    4|\n",
      "|  0.0|       0.0| 2668|\n",
      "|  0.0|       2.0|    8|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_4.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.8391297536016666\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_4 = acc_eval.evaluate(test_results_4)\n",
    "print('Accuracy of model at predicting: {}'.format(acc_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_5 = rfc.fit(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_5 = predictor_5.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 2676|\n",
      "|  1.0|  234|\n",
      "|  2.0|  129|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_5.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|  102|\n",
      "|  1.0|       0.0|  219|\n",
      "|  2.0|       2.0|   27|\n",
      "|  1.0|       2.0|   15|\n",
      "|  0.0|       0.0| 2655|\n",
      "|  0.0|       2.0|   21|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_5.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.8392095041530172\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_5 = acc_eval.evaluate(test_results_5)\n",
    "print('Accuracy of model at predicting: {}'.format(acc_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Higher accuracy and better result. But not very good!\n",
    "- Find another solution to improve performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
